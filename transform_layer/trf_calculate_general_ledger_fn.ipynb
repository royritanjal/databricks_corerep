{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46398c35-d7e1-4ef0-a0a4-308042e9c97a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create new schema : transform_layer"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use catalog corerep;\n",
    "create schema if not exists transform_layer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871a4a05-02e1-40c5-baef-bb0a5b276361",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create re-usable transformation"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Optional\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.column import Column\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "def transform_to_silver_gl(\n",
    "    df_in: DataFrame,\n",
    "    source_system: str,\n",
    "    *,\n",
    "    # 1) Select + cast + rename (schema mapping)\n",
    "    select_exprs: List[Column],\n",
    "\n",
    "    # 2) DQ filters\n",
    "    not_null_cols: List[str],\n",
    "\n",
    "    # 3) Group and aggregate\n",
    "    group_by_cols: List[str],\n",
    "    sum_cols: List[str],  # columns to sum\n",
    "    net_movement_expr: Column,  # expression that creates Net_Movement\n",
    "\n",
    "    # 4) De-duplication (deterministic)\n",
    "    dedup_key: List[str],\n",
    "    dedup_order: List[Tuple[str, str]],  # e.g. [(\"ParentSystemId\",\"desc\")]\n",
    "\n",
    "    # 5) Closing balance window\n",
    "    closing_partition_cols: List[str],\n",
    "    closing_order_col: str,  # e.g. \"PeriodKey\"\n",
    "\n",
    "    # Optional: add more calculated cols after aggregation\n",
    "    post_calc_cols: Optional[Dict[str, Column]] = None,\n",
    ") -> DataFrame:\n",
    "\n",
    "    # Step 0: audit\n",
    "    df = (df_in\n",
    "          .withColumn(\"_ingest_ts\", F.current_timestamp())\n",
    "          .withColumn(\"_source_system\", F.lit(source_system)))\n",
    "\n",
    "    # Step 1: select/cast/rename\n",
    "    df = df.select(*select_exprs)\n",
    "\n",
    "    # Step 2: DQ filters\n",
    "    cond = None\n",
    "    for c in not_null_cols:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"DQ column '{c}' not found. Available: {df.columns}\")\n",
    "        cnd = F.col(c).isNotNull()\n",
    "        cond = cnd if cond is None else (cond & cnd)\n",
    "    df = df.filter(cond)\n",
    "\n",
    "    # Step 3: aggregate\n",
    "    for c in group_by_cols:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"group_by column '{c}' not found. Available: {df.columns}\")\n",
    "    for c in sum_cols:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"sum column '{c}' not found. Available: {df.columns}\")\n",
    "\n",
    "    agg_exprs = [F.sum(c).alias(c) for c in sum_cols]\n",
    "    df = df.groupBy(*group_by_cols).agg(*agg_exprs)\n",
    "\n",
    "    # Add Net_Movement\n",
    "    df = df.withColumn(\"Net_Movement\", net_movement_expr)\n",
    "\n",
    "    # Optional extra derived cols after aggregation (e.g., PeriodKey)\n",
    "    if post_calc_cols:\n",
    "        for name, expr in post_calc_cols.items():\n",
    "            df = df.withColumn(name, expr)\n",
    "\n",
    "    # Step 4: dedupe (keep latest)\n",
    "    for k in dedup_key:\n",
    "        if k not in df.columns:\n",
    "            raise ValueError(f\"dedup key '{k}' not found. Available: {df.columns}\")\n",
    "\n",
    "    order_exprs = []\n",
    "    for col_name, direction in dedup_order:\n",
    "        if col_name not in df.columns:\n",
    "            raise ValueError(f\"dedup order col '{col_name}' not found. Available: {df.columns}\")\n",
    "        if direction.lower() == \"desc\":\n",
    "            order_exprs.append(F.col(col_name).desc())\n",
    "        elif direction.lower() == \"asc\":\n",
    "            order_exprs.append(F.col(col_name).asc())\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid direction '{direction}' for '{col_name}'. Use 'asc' or 'desc'.\")\n",
    "\n",
    "    w = Window.partitionBy(*dedup_key).orderBy(*order_exprs)\n",
    "    df = (df.withColumn(\"_rn\", F.row_number().over(w))\n",
    "            .filter(F.col(\"_rn\") == 1)\n",
    "            .drop(\"_rn\"))\n",
    "\n",
    "    # Step 5: Closing balance (running sum by closing_order_col desc)\n",
    "    for c in closing_partition_cols:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"closing partition col '{c}' not found. Available: {df.columns}\")\n",
    "    if closing_order_col not in df.columns:\n",
    "        raise ValueError(f\"closing order col '{closing_order_col}' not found. Available: {df.columns}\")\n",
    "\n",
    "    w2 = (Window.partitionBy(*closing_partition_cols)\n",
    "                .orderBy(F.col(closing_order_col).desc())\n",
    "                .rowsBetween(Window.unboundedPreceding, Window.currentRow))\n",
    "\n",
    "    df = df.withColumn(\"Closing_Balance\", F.sum(\"Net_Movement\").over(w2))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a21b6c-a39d-412f-9dc5-771ef9f17119",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display data for newly created transform function"
    }
   },
   "outputs": [],
   "source": [
    "df_transform_gl_account = spark.table(\"corerep.raw_data.raw_gl_balances\")\n",
    "\n",
    "select_exprs = [\n",
    "    F.col(\"ParentSystemId\").cast(\"bigint\").alias(\"ParentSystemId\"),\n",
    "    F.col(\"GLBalanceKey\").cast(\"string\").alias(\"GLBalanceKey\"),\n",
    "    F.col(\"LedgerId\").alias(\"LedgerId\"),\n",
    "    F.col(\"TransactionCurrency\").alias(\"TransactionCurrency\"),\n",
    "    F.col(\"LedgerCurrency\").cast(\"string\").alias(\"LedgerCurrency\"),\n",
    "    F.col(\"PeriodYear\").alias(\"PeriodYear\"),\n",
    "    F.col(\"PeriodNum\").alias(\"PeriodNum\"),\n",
    "    F.col(\"TransCurrPeriodNetDr\"),\n",
    "    F.col(\"TransCurrPeriodNetCr\"),\n",
    "]\n",
    "\n",
    "df_general_ledger = transform_to_silver_gl(\n",
    "    df_in=df_transform_gl_account,\n",
    "    source_system=\"raw_gl_balances\",\n",
    "\n",
    "    # Step 1\n",
    "    select_exprs=select_exprs,\n",
    "\n",
    "    # Step 2\n",
    "    not_null_cols=[\"ParentSystemId\", \"GLBalanceKey\"],\n",
    "\n",
    "    # Step 3\n",
    "    group_by_cols=[\n",
    "        \"ParentSystemId\", \"GLBalanceKey\", \"LedgerId\",\n",
    "        \"TransactionCurrency\", \"LedgerCurrency\", \"PeriodYear\", \"PeriodNum\"\n",
    "    ],\n",
    "    sum_cols=[\"TransCurrPeriodNetDr\", \"TransCurrPeriodNetCr\"],\n",
    "    net_movement_expr=F.col(\"TransCurrPeriodNetDr\") - F.col(\"TransCurrPeriodNetCr\"),\n",
    "\n",
    "    # PeriodKey after aggregation (same as your example)\n",
    "    post_calc_cols={\"PeriodKey\": F.col(\"PeriodYear\") * 100 + F.col(\"PeriodNum\")},\n",
    "\n",
    "    # Step 4\n",
    "    dedup_key=[\"GLBalanceKey\"],\n",
    "    dedup_order=[(\"ParentSystemId\", \"desc\")],\n",
    "\n",
    "    # Step 5\n",
    "    closing_partition_cols=[\n",
    "        \"ParentSystemId\", \"GLBalanceKey\", \"LedgerId\",\n",
    "        \"TransactionCurrency\", \"LedgerCurrency\", \"PeriodYear\"\n",
    "    ],\n",
    "    closing_order_col=\"PeriodKey\",\n",
    ")\n",
    "\n",
    "df_general_ledger.display()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8166643144103196,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "trf_calculate_general_ledger_fn",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
